{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python env: modnenv_v2\n",
    "\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path, PosixPath\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from modnet.models import EnsembleMODNetModel\n",
    "from modnet.preprocessing import MODData\n",
    "from modnet.hyper_opt import FitGenetic\n",
    "from monty.serialization import dumpfn, loadfn\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "from pymatgen.core.structure import Structure\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error \n",
    "from scipy.stats import spearmanr\n",
    "from IPython.display import Image\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'acquilib' from '/home/vtrinquet/Documents/Doctorat/JNB_Scripts_Clusters/NLO/HT/ref_idx/re2fractive/artificial/acquilib.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import modwrap as mdw\n",
    "\n",
    "reload(mdw)\n",
    "\n",
    "import acquilib as acq\n",
    "\n",
    "reload(acq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_re2f = Path('/home/vtrinquet/Documents/Doctorat/JNB_Scripts_Clusters/NLO/HT/ref_idx/re2fractive')\n",
    "\n",
    "# Load the featurized MODData\n",
    "md_featselec = (path_re2f / 'humanguided' / 'v0' / 'mod.data_refeatselec_v0_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-23 16:05:12,918 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f00f7a0a280> object, created with modnet version 0.4.1\n"
     ]
    }
   ],
   "source": [
    "md = MODData.load(md_featselec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.df_featurized = md.df_featurized.iloc[:54]\n",
    "md.df_targets = md.df_targets.iloc[:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists!\n",
      "2024-01-23 16:09:00,598 - modnet - INFO - Loaded <modnet.models.ensemble.EnsembleMODNetModel object at 0x7f00e5bf2040> object, created with modnet version 0.4.1\n",
      "Model already exists!\n",
      "2024-01-23 16:09:00,703 - modnet - INFO - Loaded <modnet.models.ensemble.EnsembleMODNetModel object at 0x7f00ffcf2340> object, created with modnet version 0.4.1\n"
     ]
    }
   ],
   "source": [
    "model_params={\n",
    "    'size_pop':2, # dflt 20\n",
    "    'num_generations':2, # dflt 10\n",
    "    'nested':0, # dflt = 5\n",
    "    'n_jobs':2,\n",
    "    'early_stopping':2, # dflt 4\n",
    "    'refit':5, # dflt = 5\n",
    "    'fast':False,\n",
    "    }\n",
    "\n",
    "Xt, Yt, Xp, Yp, results, model, scores_bk = mdw.actilearn(\n",
    "    structures=None,\n",
    "    ids=None,\n",
    "    X=None,\n",
    "    Y=None,\n",
    "    md_feat=None,\n",
    "    md_featselec=md,\n",
    "    start_frac=None,\n",
    "    start_n=50,\n",
    "    start_set=None,\n",
    "    start_state=42,\n",
    "    ncycles=2,\n",
    "    accuracy=None,\n",
    "    accuracy_type=None,\n",
    "    end_set=None,\n",
    "    model_type=FitGenetic,\n",
    "    model_params=model_params,\n",
    "    cv_k=2,\n",
    "    cv_state=42,\n",
    "    acquisition=acq.exploration,\n",
    "    acquisition_kwargs=None,\n",
    "    acquisition_n=10,\n",
    "    acquisition_frac=None,\n",
    "    featurize_cycle=None,\n",
    "    featurize_cv=None,\n",
    "    featselec_cycle=None,\n",
    "    featselec_cv=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "value1\n",
      "p\n",
      "value2\n"
     ]
    }
   ],
   "source": [
    "def rankt(arg1, arg2='meh', **kwargs):\n",
    "    # Implementation of rankt function\n",
    "    # Use arg1, arg2, and kwargs as needed\n",
    "    print(arg1)\n",
    "    pass\n",
    "\n",
    "def rankp(arg1, arg2='meh', arg3='3', **kwargs):\n",
    "    # Implementation of rankp function\n",
    "    # Use arg1, arg2, arg3, and kwargs as needed\n",
    "    print(arg3)\n",
    "    pass\n",
    "\n",
    "def select(rank_function, tmp, **kwargs):\n",
    "    # Call the specified rank function with arguments and kwargs\n",
    "    print(tmp)\n",
    "    return rank_function(**kwargs)\n",
    "\n",
    "# Example usage:\n",
    "kwargs_for_rank = {'arg1': 'value1', 'arg3': 'value2'}\n",
    "\n",
    "# Using select with rankt\n",
    "result_t = select(rankt, tmp='t', **kwargs_for_rank)\n",
    "\n",
    "# Using select with rankp\n",
    "result_p = select(rankp, tmp='p', **kwargs_for_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already analyzed!\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def analysis(\n",
    "      scores_dir_path=Path('./benchmark/scores')\n",
    "      ):\n",
    "\n",
    "    if (scores_dir_path / f\"scores_overall.png\").exists() and\\\n",
    "       (scores_dir_path / f\"scores_overall.pdf\").exists() and\\\n",
    "       (scores_dir_path / f\"scores_unc_overall.png\").exists() and\\\n",
    "       (scores_dir_path / f\"scores_unc_overall.pdf\").exists():\n",
    "       print('Already analyzed!')\n",
    "       return\n",
    "\n",
    "\n",
    "    scores_all = {\n",
    "       'mae_folds': [],\n",
    "       'rmse_folds': [],\n",
    "       'spr_folds': [],\n",
    "       'mae_unc_folds': [],\n",
    "       'rmse_unc_folds': [],\n",
    "       'mae_avg': [],\n",
    "       'rmse_avg': [],\n",
    "       'spr_avg': [],\n",
    "       'mae_unc_avg': [],\n",
    "       'rmse_unc_avg': [],\n",
    "    }\n",
    "\n",
    "    scoresfiles = [f for f in listdir(scores_dir_path) if isfile(join(scores_dir_path, f)) and any(ch.isdigit() for ch in f)]\n",
    "    for f in scoresfiles:\n",
    "        scores_path = (scores_dir_path / f)\n",
    "\n",
    "        with open(scores_path) as f:\n",
    "            scores = json.load(f)\n",
    "\n",
    "        scores_all['mae_folds'].append(scores['pred_mae'])\n",
    "        scores_all['rmse_folds'].append(scores['pred_rmse'])\n",
    "        scores_all['spr_folds'].append(scores['pred_spr'])\n",
    "        scores_all['mae_unc_folds'].append(scores['unc_mae'])\n",
    "        scores_all['rmse_unc_folds'].append(scores['unc_rmse'])\n",
    "        scores_all['mae_avg'].append(np.mean(scores['pred_mae']))\n",
    "        scores_all['rmse_avg'].append(np.mean(scores['pred_rmse']))\n",
    "        scores_all['spr_avg'].append(np.mean(scores['pred_spr']))\n",
    "        scores_all['mae_unc_avg'].append(np.mean(scores['unc_mae']))\n",
    "        scores_all['rmse_unc_avg'].append(np.mean(scores['unc_rmse']))\n",
    "    \n",
    "    x = range(len(scores_all['mae_avg']))\n",
    "\n",
    "    # Create subplots and unpack the Axes object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the data using the ax object\n",
    "    ax.plot(x, -np.array(scores_all['mae_avg']), label='-MAE')\n",
    "    ax.plot(x, -np.array(scores_all['rmse_avg']), label='-RMSE')\n",
    "    ax.plot(x, scores_all['spr_avg'], label='+SPR')\n",
    "\n",
    "    # Set labels for the axes\n",
    "    ax.set_xlabel('# AL cycles', fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig((scores_dir_path / f\"scores_overall.png\"))\n",
    "    fig.savefig((scores_dir_path / f\"scores_overall.pdf\"))\n",
    "\n",
    "    # Create subplots and unpack the Axes object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the data using the ax object\n",
    "    ax.plot(x, -np.array(scores_all['mae_unc_avg']), label='-MAE')\n",
    "    ax.plot(x, -np.array(scores_all['rmse_unc_avg']), label='-RMSE')\n",
    "\n",
    "    # Set labels for the axes\n",
    "    ax.set_xlabel('# AL cycles', fontsize=14)\n",
    "    ax.set_ylabel('Score uncertainty', fontsize=14)\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend(fontsize=12)\n",
    "    fig.savefig((scores_dir_path / f\"scores_unc_overall.png\"))\n",
    "    fig.savefig((scores_dir_path / f\"scores_unc_overall.pdf\"))\n",
    "\n",
    "    return scores_all\n",
    "    \n",
    "_ = analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test1': [], 'test2': []}\n",
      "{'test1': [5], 'test2': [5]}\n"
     ]
    }
   ],
   "source": [
    "test = {'test1': [], 'test2': []}\n",
    "print(test)\n",
    "test['test1'].append(5)\n",
    "test['test2'].append(5)\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
